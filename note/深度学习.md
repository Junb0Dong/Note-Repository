# 深度学习

## 预备知识

### 线性回归+基础优化算法

#### 线性模型

- 给定n维输入$x = [x_1,x_2,\dots,x_n]^T$

- 线性模型有一个n维权重和一个标量偏差
  $$
  w = [w_1,w_2,\dots,w_n]^T,b
  $$

- 输出是输入的加权平方和
  $$
  y=w_1x_1+w_2x_2+\dots+w_nx_n+b
  $$

- 向量版本：$y=\langle w,x\rangle+b$

#### 衡量预估质量

- 比较真实值和预估值
  $$
  \ell(y,\hat y) = \frac{1}{2}(y- \hat y)^2
  $$

  > y：真实值
  >
  > $\hat y$：估计值
  >
  > $\frac{1}{2}$：微分时候可约
  >
  > 这个叫做平方损失(MSE)

#### 训练数据

- 通常越多越好

- 假设我们有n个样本，记
  $$
  X = [x_1,x_2,\dots,x_n]^T \quad y = [y_1,y_2,\dots,y_n]^T  
  $$

- 训练损失
  $$
  \ell(X,y,w,b)=\frac{1}{2n}\sum_{i=1}^n(y_i-\langle x_i,w\rangle-b)^2=\frac{1}{2n}||y-Xw-b||^2
  $$

- 最小化损失来学习参数
  $$
  \mathbf{w}^*, b^* = \arg \min_{w, b} \, \ell(\mathbf{X}, \mathbf{y}, \mathbf{w}, b)
  $$
  

**显示解**

> 数据结构简单，可以显示的带入公式求解

- 将偏差加入权重
  $X \leftarrow [X, 1] \quad w \leftarrow \begin{bmatrix} \mathbf{w} \\ b \end{bmatrix} $

- 损失函数  
  $$
  \ell(X, \mathbf{y}, \mathbf{w}) = \frac{1}{2n} \left\| \mathbf{y} - X \mathbf{w} \right\|^2
  $$

- 梯度计算
  $$
  \frac{\partial}{\partial \mathbf{w}} \ell(X, \mathbf{y}, \mathbf{w}) = \frac{1}{n} ( \mathbf{y} - X \mathbf{w} )^T X
  $$

- 损失是凸函数，所以最优解满足  
  $$
  \frac{\partial}{\partial \mathbf{w}} \ell(X, \mathbf{y}, \mathbf{w}) = 0
  \Rightarrow \frac{1}{n} ( \mathbf{y} - X \mathbf{w} )^T X = 0
  \Rightarrow \mathbf{w}^* = (X^T X)^{-1} X \mathbf{y}
  $$

> 最后的$(X^T X)^{-1} X \mathbf{y}$就是显示解
>
> 矩阵求导法则$\frac{dA^2}{dx}=2A^T$
>
> 最后一等式我推导出来是$(X^T X)^{-1} X^T \mathbf{y}$

#### 总结

- 线性回归是对n维输入的加权，外加偏差
- 使用MSE来衡量预测值和真实值的差异
- 线性回归有显示解
- 线性回归可以看作是单层神经网络