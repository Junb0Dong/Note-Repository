# 仿真代码

## 强化学习基础仿真

### 目前已掌握：

- 仿真环境的运用

- env的使用和env的代码解读

  > 参数的设置（reward, observation）

- Policy Loop

  ```python
  # this example assumes an env has already been created, and performs one agent rollout
  import numpy as np
  
  def get_policy_action(obs):
      # a trained policy could be used here, but we choose a random action
      low, high = env.action_spec
      return np.random.uniform(low, high)
  
  # reset the environment to prepare for a rollout
  obs = env.reset()
  
  done = False
  ret = 0.
  while not done:
      action = get_policy_action(obs)         # use observation to decide on an action
      obs, reward, done, _ = env.step(action) # play action
      ret += reward
  print("rollout completed with return {}".format(ret))
  ```

### 还差：

- policy的设计
- robosuite没有内置的强化学习策略实现，但与主流的强化学习库有很好的兼容（`stable-baselines3`）

**几个强化学习开源库**：

- [RLkit](https://github.com/rail-berkeley/rlkit)

  灵活、可拓展，设和实验性和多样化任务

- [SKRL-Reinforcement Learning](https://skrl.readthedocs.io/en/latest/index.html)

  基于TensorFlow，可以不用看了

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)

  文档完善，基于Pytorch

- OpenAI Baselines

## OpenAI gym代码学习

### Training an Agent

the law about picking an action: epsilon-greedy strategy, where we pick a random action with the percentage `epsilon` and the greedy action (currently valued as the best) `1 - epsilon`.

---

## 总结上一阶段

- 尝试了`robosuite`, `openai gym`, `stable baseline3`，都是很好的库，但对于我想实现的基于机械臂的强化学习来说，没有现成的interface或者API，如果调用要花费很大功夫，于是**放弃。**
- 熟悉了这三个库之后，对强化学习的环境倒是有了新的认识，这算是积极的一面吧。
- 总的来说，算是失措，不过还好，比之前要好太多了。现在学会了阅读documentation，也探索了更多别的库。

---

## 新阶段

- 使用`Isaac lab`和`skrl`库初窥强化学习

## SKRL API 

### Agents

Agents are autonomous entities that interact with the environment to learn and improve their behavior. **Agents’ goal is to learn an optimal policy**, which is a correspondence between states and actions that maximizes the cumulative reward received from the environment over time.

> 一个动作的执行器，执行策略并与环境进行交互（类似中间值），目的是为了学习到一个最优策略

### Environments

The environment plays a fundamental and crucial role in defining the RL setup. It is the place where the agent interacts, and it is responsible for **providing the agent with information about its current state, as well as the rewards/penalties associated with each action.**

> 提供环境信息信息，包括环境的基础设置，state and rewards with each action

### Memories

Memories are storage components that allow agents to collect and use/reuse current or past experiences of their interaction with the environment or other types of information.

> 保存并重复利用之间的数据

### Models

Models (or agent models) refer to a representation of the **agent’s policy**, value function, etc. that the agent uses to make decisions. Agents can have one or more models, and their parameters are adjusted by the optimization algorithms.

> policy的寄生体或者叫实例



