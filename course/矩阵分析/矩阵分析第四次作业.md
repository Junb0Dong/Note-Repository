# 矩阵分析第四次作业-董骏博-12432995

> NAME：董骏博
>
> SID：12432995

## 任务1

将（2）带入（1）得
$$
\min_x(f(x)-z)^TW^{-1}(f(x)-z) = \min_x(J_kd_k+f(x_k)-z)^TW^{-1}(J_kd_k+f(x_k)-z)
$$
展开上式可得：
$$
 (f(x_k) - z)^T W^{-1}(f(x_k) - z)+2(f(x_k) - z)^T W^{-1}J_k d_k + d_k^T J_k^T W^{-1}J_k d_k
$$
为找到$d_k$的最小二乘，对$d_k$求导并令其导数为0可得：
$$
2J_k^T W^{-1}(f(x_k) - z)+2J_k^T W^{-1}J_k d_k = 0
$$
其中将$f(x_k) - z = r_k$带入得
$$
J_k^T W^{-1}J_k d_k = -J_k^T W^{-1}r_k
$$

所以
$$
d_k = -(J_k^TW^{-1}J_k)^{-1}J_k^TW^{-1}r_k
$$
### `GN_Solver.m`中**matlab代码修改部分**

```matlab
A = J'*pinv(g.W)*J; % dk的系数，求伪逆，否则会报错
b = -J'*pinv(g.W)*r; % 等式右边
dk = A\b; % 求解dk
```

## 任务2

$$
\begin{split}
E_{ij}& =\frac{\partial T_{i,j}}{\partial[x_i,\tau_{i,1},\delta_{i,1}]^T}\\
&=\left[\frac{\partial T_{i,j}}{\partial x_i},\frac{\partial T_{i,j}}{\partial\tau_{i,1}},\frac{\partial T_{i,j}}{\partial\delta_{i,1}}\right]^T
\end{split}
$$

其中：
$$
T_{i,j} = \frac{||x_i-s_j||-||x_1-s_j||}{c} + \tau_{i,1}+\delta_{i,1}\Delta t_j
$$

接下来分别求$\frac{\partial T_{i,j}}{\partial x_i}$、$\frac{\partial T_{i,j}}{\partial T_{i,1}}$和$\frac{\partial T_{i,j}}{\partial\delta_{i,1}}$：
   - 求$\frac{\partial T_{i,j}}{\partial x_i}$：
     - 首先，$T_{i,j}=\frac{\left\|\mathbf{x}_i - \mathbf{s}_j\right\|-\left\|\mathbf{x}_1-\mathbf{s}_j\right\|}{c}+T_{i,1}+\delta_{i,1}\Delta t_j$。
     - 令$a=\mathbf{x}_i - \mathbf{s}_j$，$b=\mathbf{x}_1-\mathbf{s}_j$，则$T_{i,j}=\frac{\|a\|-\|b\|}{c}+T_{i,1}+\delta_{i,1}\Delta t_j$。
     - 根据链式法则，$\frac{\partial T_{i,j}}{\partial x_i}=\frac{1}{c}\frac{\partial\left(\|a\|-\|b\|\right)}{\partial x_i}$。
     - 对于$\frac{\partial\|a\|}{\partial x_i}$，我们有$\|a\|=\sqrt{a^T a}$，$\frac{\partial\|a\|}{\partial x_i}=\frac{a^T}{\|a\|}$（$a=\mathbf{x}_i - \mathbf{s}_j$）。
     - 而$\frac{\partial\|b\|}{\partial x_i}=0$。
     - 所以$\frac{\partial T_{i,j}}{\partial x_i}=\frac{1}{c}\frac{\left(\mathbf{x}_i - \mathbf{s}_j\right)^T}{\left\|\mathbf{x}_i - \mathbf{s}_j\right\|}$。
   - 求$\frac{\partial T_{i,j}}{\partial T_{i,1}}$：
     - 由$T_{i,j}=\frac{\left\|\mathbf{x}_i - \mathbf{s}_j\right\|-\left\|\mathbf{x}_1-\mathbf{s}_j\right\|}{c}+T_{i,1}+\delta_{i,1}\Delta t_j$，直接求导得$\frac{\partial T_{i,j}}{\partial T_{i,1}} = 1$。
   - 求$\frac{\partial T_{i,j}}{\partial\delta_{i,1}}$：
     - 由$T_{i,j}=\frac{\left\|\mathbf{x}_i - \mathbf{s}_j\right\|-\left\|\mathbf{x}_1-\mathbf{s}_j\right\|}{c}+T_{i,1}+\delta_{i,1}\Delta t_j$，直接求导得$\frac{\partial T_{i,j}}{\partial\delta_{i,1}}=\Delta t_j$。


$$
\begin{cases}
\frac{\partial T_{i,j}}{\partial x_{i}}=\frac{1}{c} \frac{\partial}{\partial x_{i}} (\left\lVert X_i - s_j \right\rVert - \left\lVert X_1 - s_j \right\rVert) = \frac{1}{c}\frac{\left(\mathbf{x}_i - \mathbf{s}_j\right)^T}{\left\|\mathbf{x}_i - \mathbf{s}_j\right\|} \\
\frac{\partial T_{i,j}}{\partial \tau_{i,1}} = 1 \\
\frac{\partial T_{i,j}}{\partial\delta_{i,1}}=\Delta t_j
\end{cases}
$$

将结果代入$E_{i,j}$的表达式：
$$
E_{i,j}=\left[\begin{matrix}\frac{1}{c}\frac{\left(\mathbf{x}_i - \mathbf{s}_j\right)^T}{\left\|\mathbf{x}_i - \mathbf{s}_j\right\|}\\1\\\Delta t_j\end{matrix}\right]
$$

$$
G_{ij} = \frac{\partial T_{i,j}}{\partial s_j}
$$

1. 分别对$T_{i,j}$中的各项求偏导数：
   - 令$a = \mathbf{x}_i - \mathbf{s}_j$，$b=\mathbf{x}_1 - \mathbf{s}_j$。

   - 则$T_{i,j}=\frac{\|a\|-\|b\|}{c}+\tau_{i,1}+\delta_{i,1}\Delta t_j$。

   - 对于$\frac{\|a\|-\|b\|}{c}$这一项求偏导数：
     - 根据链式法则
       $$
       \frac{\partial}{\partial\mathbf{s}_j}\left(\frac{\|a\|-\|b\|}{c}\right)=\frac{1}{c}\left(\frac{\partial\|a\|}{\partial\mathbf{s}_j}-\frac{\partial\|b\|}{\partial\mathbf{s}_j}\right)
       $$
       
     - 对于$\frac{\partial\|a\|}{\partial\mathbf{s}_j}$，$\|a\|=\sqrt{a^T a}$，$\frac{\partial\|a\|}{\partial\mathbf{s}_j}=-\frac{a^T}{\|a\|}$（$a = \mathbf{x}_i - \mathbf{s}_j$）。
     
     - 同理，对于$\frac{\partial\|b\|}{\partial\mathbf{s}_j}$，$\|b\|=\sqrt{b^T b}$，$\frac{\partial\|b\|}{\partial\mathbf{s}_j}=-\frac{b^T}{\|b\|}$（$b=\mathbf{x}_1 - \mathbf{s}_j$）。
     
     - 所以
       $$
       \frac{\partial}{\partial\mathbf{s}_j}\left(\frac{\|a\|-\|b\|}{c}\right)=\frac{1}{c}\left(-\frac{(\mathbf{x}_i - \mathbf{s}_j)^T}{\left\|\mathbf{x}_i - \mathbf{s}_j\right\|}+\frac{(\mathbf{x}_1 - \mathbf{s}_j)^T}{\left\|\mathbf{x}_1 - \mathbf{s}_j\right\|}\right)
       $$
       
     
   - 对于$\tau_{i,1}$和$\delta_{i,1}\Delta t_j$这两项，它们与$\mathbf{s}_j$无关，所以它们对$\mathbf{s}_j$的偏导数为$0$。
   
2. 综上，$G_{ij}$的表达式为：
   $$
   G_{ij}=\frac{1}{c}\left(-\frac{(\mathbf{x}_i - \mathbf{s}_j)^T}{\left\|\mathbf{x}_i - \mathbf{s}_j\right\|}+\frac{(\mathbf{x}_1 - \mathbf{s}_j)^T}{\left\|\mathbf{x}_1 - \mathbf{s}_j\right\|}\right)
   $$

### `compute_J.m`中**matlab代码修改部分**

```matlab
% Eij的计算
Eij = [dx' / norm(dx) / g.cc; 1; sum(g.dt(1:j))]
% Gij的计算
dx1 = x1_loc - s_loc
Gij = (1/g.cc) * (-dx' / norm(dx) + dx1' / norm(dx1));
% r_todoj = T_ij - t_ij
T_ij=1/g.cc*(norm(dx)-norm(g.x(1,1:3)-s_loc))+off+dri*sum(g.dt(1:j));
r_tdoaij=T_ij-g.tdoa(i-1,j);
% r_odoj = s_j+1 - s_j - m_j
r_odoj = s_next - s_now - m_j;
```

## 任务三

### 实验结果分析

实验结果如下：

```matlab
init_sigma= 0.500000 ， Mic. Loc. err.: 0.054150 m
init_sigma= 1.000000 ， Mic. Loc. err.: 0.794560 m
init_sigma= 2.000000 ， Mic. Loc. err.: 2.080897 m
```

从计算出的平均误差可以看出，随着初值噪声标准差$\sigma_{init}$的增大（从$0.5$到$2$），平均麦克风位置估计误差也逐渐增大。这表明初值的选取对非线性最小二乘的估计结果有显著影响。

当$\sigma_{init}=0.5$时，噪声标准差相对较小，说明最优解在初值附近，算法能够更快地收敛到较好的解；而当 $\sigma_{init}=2$ 时，较大的噪声标准差可能使非线性最小二乘更容易陷入局部最优解，或者需要更多的迭代次数才能接近全局最优解，从而导致平均误差增大。

### 解释：为什么传统非线性优化结果依赖初值？

传统非线性优化方法是基于当前估计值（初值）的局部信息（如梯度和雅可比矩阵）进行迭代更新，迭代过程可能会陷入局部极小值或鞍点，无法找到全局最优解。

非线性函数存在多局部最优，初值决定收敛到的局部最优。若选在局部最优而非全局最优附近，结果非全局最佳。比如在这次作业中，标定和定位问题是使用非线性最小二乘方法进行求解，可能存在多个局部最优解，使定位和标定精度受限，因此不同的初值可能会收敛迭代到不同的局部最优区域。

## 作业代码

作业代码一共四个文件

`compute_J.m`：修改了`Eij`,`Gij`,`r_odoj`,`r_todoj`

`GN_Solver.m`：修改了`dk`的计算

`main.m`：保留了原始的`main`函数代码，未作修改

`task3.m`：根据**task3**的要求修改`main`函数
