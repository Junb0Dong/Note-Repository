# AAIå¤ä¹ 

## Lecture 2 Basic Search

![image-20250101135138075](image/image-20250101135138075.png)

## Lecture 3 Heuristic Search

Q: Waht is the difference between an evaluation function and a heuristic function?

A: Heuristic is a component of an evaluation function. / Evluation function consists of heuristic(s).

### A* Search

Expand the node s that has the minmal $f(s) = h(s)+g(s)$

- $g(s)$: cost from Start to s.

- $h(s)$: <u>estimated</u> cost from s to Goal.

  > sometimes using the straight-line distance or mahattan distance

- $f(s)$: <u>estimated</u> total cost of path from Start through s to Goal. 

<img src="image/image-20250101143957204.png" alt="image-20250101143957204" style="zoom:40%;" />

**Summary**: Search Methods with $f(s)$

- Uniform-cost search: $f(s) = g(s)$
- Greedy best-first search: $f(s) = h(s)$
- A* search: $f(s)= g(s) + h(s)$

> g(s): the path cost from Start to noed s.
>
> h(s): the estimated cost from nodes to Goal.

**Generate admissible heuristics:**

- from relaxed problems
- from sub-problem
- from experience

### Exercise

Heuristic Path Algorithm is a type of Best First Search and its evaluation function is $f(n) = (2-w)g(n) + w^*h(n)$

1) Suppose h is admissable. How to set w to make it optimal?
   $$
   (2 - w)g(n)+w \cdot h(n)=g(n)+h(n) \\
   2g(n)-wg(n)+w \cdot h(n)=g(n)+h(n) \\
   2g(n)-wg(n)=g(n) \quad \text{and} \quad w \cdot h(n)=h(n) \\
   2 - w = 1 \quad \text{and} \quad w = 1
   $$
   We get making it optimal when w=1.

2. When w=0, w=1, and w=2, which search algorithm does it represent? Explain your answer. 

   å½“ $ w = 0 $ æ—¶ï¼š
   - è¯„ä¼°å‡½æ•° $ f(n)=(2 - 0)g(n)+0 \cdot h(n)=2g(n) $ã€‚
   - è¿™ä»£è¡¨äº†Dijkstraç®—æ³•ï¼Œå› ä¸ºDijkstraç®—æ³•åªè€ƒè™‘ä»èµ·ç‚¹åˆ°å½“å‰èŠ‚ç‚¹çš„å®é™…ä»£ä»· $ g(n) $ï¼Œè€Œä¸è€ƒè™‘å¯å‘å¼ä¼°è®¡ $ h(n) $ã€‚æ­¤æ—¶ï¼Œ$ f(n) $ åªä¸ $ g(n) $ æœ‰å…³ï¼Œä¸”ç³»æ•°ä¸º2ï¼ˆåœ¨Dijkstraç®—æ³•ä¸­ï¼Œç³»æ•°ä¸å½±å“æœç´¢ç­–ç•¥çš„æœ¬è´¨ï¼‰ã€‚

   å½“ $ w = 1 $ æ—¶ï¼š
   - è¯„ä¼°å‡½æ•° $ f(n)=(2 - 1)g(n)+1 \cdot h(n)=g(n)+h(n) $ã€‚
   - è¿™ä»£è¡¨äº†A\*ç®—æ³•ï¼Œå› ä¸ºA\*ç®—æ³•çš„è¯„ä¼°å‡½æ•°å°±æ˜¯ $ f(n)=g(n)+h(n) $ã€‚

   å½“ $ w = 2 $ æ—¶ï¼š
   - è¯„ä¼°å‡½æ•° $ f(n)=(2 - 2)g(n)+2 \cdot h(n)=2h(n) $ã€‚
     - è¿™ä»£è¡¨äº†è´ªå¿ƒæœ€ä½³ä¼˜å…ˆæœç´¢ï¼ˆGreedy Best - First Searchï¼‰ç®—æ³•ï¼Œå› ä¸ºè´ªå¿ƒæœ€ä½³ä¼˜å…ˆæœç´¢åªè€ƒè™‘å¯å‘å¼ä¼°è®¡ $ h(n) $ï¼Œè€Œä¸è€ƒè™‘ä»èµ·ç‚¹åˆ°å½“å‰èŠ‚ç‚¹çš„å®é™…ä»£ä»· $ g(n) $ã€‚æ­¤æ—¶ï¼Œ$ f(n) $ åªä¸ $ h(n) $ æœ‰å…³ï¼Œä¸”ç³»æ•°ä¸º2ï¼ˆåœ¨è´ªå¿ƒæœ€ä½³ä¼˜å…ˆæœç´¢ä¸­ï¼Œç³»æ•°ä¸å½±å“æœç´¢ç­–ç•¥çš„æœ¬è´¨ï¼‰ã€‚

## Lecture 4 MetaHuristic

**What is Evaluation Algorithm?**

It is the study of computational systems which use ideas and get inspirations from natural evolution.

**Generate-and-Test: Description of steps**

1. Generate the initial solution at random and denote it as the current solution.

2. Generate the **next solution** from the current one by **perturbation.**

   > Perturbation can be `Crossover`, `Mutation`

3. Test whether the newly generated solution (next solution) is acceptable;

   - Accepted it as the current solution if yes;
   - Keep the current solution unchanged otherwise.

4. Go to Step 2 if the current solution is unsatisfactory, stop otherwise.

<img src="AAIå¤ä¹ _image/image-20250101160519536.png" alt="image-20250101160519536" style="zoom:40%;" />

Simple consturct

1. Initialize
2. Evaluation
3. Repeat until get done(find the optimal solution)

### Lab

**Modified Order Crossover(MOX)**

<img src="AAIå¤ä¹ _image/image-20250101164835543.png" alt="image-20250101164835543" style="zoom:45%;" />

**Partially-mapped crossover(PMX)**

<img src="AAIå¤ä¹ _image/image-20250101164924373.png" alt="image-20250101164924373" style="zoom:45%;" />

### Exercise

Consider a TSP problem illustrated in the following figure.

- Exercise 1 Design an appropriate crossover operator and justify your design. 

  > PMX / MOX
  >
  > All chromosomes carry exactly the same values and differ only in the ordering of these values. 
  
- Exercise 2 Design an appropriate mutation operator and justify your design.

  > swap the cities at positions i and j.
  >
  > All chromosomes carry exactly the same values and differ only in the ordering of these values. 

Two random cities in the path are selected and the positions of these two cities are swapped. **The crossover operation is to generate better offspring by preserving good gene characteristics, which is essentially to find a better solution among the existing local high-quality solutions.**

**Mutation is to change a small part of individual genes, which can jump out of the original local search space and guide the algorithm to explore and find new solution space.**

> The crossover can preserve good genes,  which can find a better solution among the existing solutions
>
> The mutation can change a small part of th individual genes, which can guide algorithm to explore new solution space.

## Lecture5 MetaheuristicsII

æµ®èºäº†ï¼Œæ²¡çœ‹ä¸‹å»

## Lecture6 Supervised Learning(I)

### Machine Learning

- Supervised learning: Training data include both inputs and outputs

  - Classification, regression

  Given:  training data of inputs $X_l$ and corresponding outputs $y_l$.

  Goal: predict a â€˜**correct**â€™ output for a new input.

- Unsupervised learning: Training data do not include outputs.

  - Clustering

  Given: only unlabeled data of inputs $X_u$

  Goal:  learn some structure of $X_u$r relationship among $X_u's$.

- Semi-supervised learning: Some training data are with output labels and some without.

  Given: A small portion of (ğ’³ğ‘™ , ğ’´ğ‘™) and large portion of ğ’³ğ‘¢.

  Goal: prediction (classification).

- Reinforcement learning: 

  Given: Training data do not include output labels, but do have a scalar feedback.

  Goal: learn a sequence of actions that maximize some cumulative rewards.

### Supervised Learning

- Using past experiences to improve future performance on some task.
- Experience: the training examples or training data. 
- What does it mean to improve performance? Learning is guided by an objective, e.g. a loss function to be minimized. 

**Generalization: Prediction Ability**

Generalization (æ³›åŒ–): the ability to produce reasonable outputs for inputs not encountered during the training process. 

**Cross Validation**

- The hold out method
- K-fold cross validation
- Leave-one-out(LOO) cross validation

### Hypothesis Space $\mathcal{H}$ for Curve Fitting

Unferfitting:  High training error and high test error.

> Use a more complex â„‹.

Overfitting:  Low training error but high test error.

> Use a less comples $\mathcal{H}$
>
> Regularization (æ­£åˆ™åŒ–): penalize certain parts of the parameter space or introduce additional constraints to constrain the hypothesis space.
>
> Get more training data

### Computational Learning Theory

<img src="AAIå¤ä¹ _image/image-20250101193918245.png" alt="image-20250101193918245" style="zoom:40%;" />

<img src="AAIå¤ä¹ _image/image-20250101194123234.png" alt="image-20250101194123234" style="zoom:45%;" />

- Bad performance on the training set (high bias)

  More complex model, different model, change hyperparameters, normalize inputs, train longer, change starting points, more complex optimization procedure, â€¦

  > åå·®æ˜¯æŒ‡æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œé«˜åå·®æ„å‘³ç€æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ä½ï¼Œå³æ¨¡å‹è¿‡äºç®€å•ï¼Œæ— æ³•æ•æ‰åˆ°æ•°æ®ä¸­çš„è§„å¾‹ã€‚

- Good performance on the training set, bad performance on the validation set (high variance)

  Simpler model, more data in the training set, regularization, feature selection, â€¦
  
  > æ–¹å·®æ˜¯æŒ‡æ¨¡å‹å¯¹ä¸åŒè®­ç»ƒæ•°æ®é›†çš„æ•æ„Ÿç¨‹åº¦ï¼Œé«˜æ–¹å·®æ„å‘³ç€æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®è¿‡åº¦æ‹Ÿåˆï¼Œæ¨¡å‹è¿‡äºå¤æ‚ï¼Œå¯¼è‡´åœ¨æ–°æ•°æ®ï¼ˆå¦‚éªŒè¯é›†ï¼‰ä¸Šçš„è¡¨ç°å¾ˆå·®ã€‚

**Gradient Descent**

<img src="AAIå¤ä¹ _image/image-20250101195954204.png" alt="image-20250101195954204" style="zoom:40%;" />
$$
W_0 = W_0 -\eta (\frac{\partial}{\partial W_0}) \\
W_1 = W_1 -\eta (\frac{\partial}{\partial W_1})
$$

## Lecture7 Supervise Learning(II)

### Linear Model

Closed-form Solution

<img src="AAIå¤ä¹ _image/image-20250101200431288.png" alt="image-20250101200431288" style="zoom:40%;" />
$$
W = (X^TX)^{-1}X^Ty
$$
Iterative Solution: Advanced

- Batch GD: update ğ’˜ once with all training samples.

  > åœ¨æ¯æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ï¼Œä¼šä½¿ç”¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†æ¥è®¡ç®—æ¢¯åº¦ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºä¸€ä¸ªåŒ…å«mä¸ªè®­ç»ƒæ ·æœ¬çš„æ•°æ®é›†ï¼Œåœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œä¼šå¯¹è¿™mä¸ªæ ·æœ¬çš„æŸå¤±å‡½æ•°æ±‚å’Œï¼Œç„¶åè®¡ç®—æ¢¯åº¦ã€‚

  Guarantee global optimum but slow.

- Stochastic GD: update ğ’˜ ğ‘ times with one training data for one update.

  > å®ƒåœ¨æ¯æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªéšæœºé€‰æ‹©çš„è®­ç»ƒæ ·æœ¬çš„æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ã€‚
  >
  > å…¶æ¢¯åº¦ä¼°è®¡çš„æ–¹å·®è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°æ‰èƒ½æ”¶æ•›ã€‚

  Fast but do not guarantee global optimum with a fixed ğ›¼.

  Online/offline settings

- Mini-batch SGD: update ğ’˜ several times with a subset of ğ’Ÿ for one update.

  > Mini-batch Gradient Descent æ˜¯ Batch GD å’Œ Stochastic GD çš„ä¸€ç§æŠ˜è¡·ã€‚å®ƒåœ¨æ¯æ¬¡æ›´æ–°å‚æ•°æ—¶ï¼Œä½¿ç”¨ä¸€å°éƒ¨åˆ†è®­ç»ƒæ ·æœ¬ï¼ˆç§°ä¸ºä¸€ä¸ª mini-batchï¼‰æ¥è®¡ç®—æ¢¯åº¦ã€‚

- Optimization:
  $$
  min_w\mathcal{L}(w)=\frac{1}{2}\sum_{n=1}^N[y^{(n)}-w^Tx^{(N)}]^2
  $$

- Gradient descent (GD): 
  $$
  w_i \leftarrow w_i + \alpha \sum_n (y_i^{(n)}-w^Tx^{(n)})x_i^{(n)}
  $$
  $\alpha$: learning rate, positive

Regularized Objective for MLR

- Regularized Objective: 
  $$
  min_w \mathcal{L}_{tr}(w) + \lambda\Omega(w)
  $$
  $\Omega(w)$: regularization

**Multivariate Linear Classification(MLC)å¤šå…ƒçº¿æ€§åˆ†ç±»**

![image-20250101212450501](AAIå¤ä¹ _image/image-20250101212450501.png)

Soft threshold function: $\sigma(z)=s(z)=\frac{1}{1+e^{-z}}$

- s(z) : sigmoid function.
- Differentiable: $s'(z) = s(z)[1-s(z)]$

### Decision Tree

Tree model: a function mapping feature vector ğ’™ to a decision ğ‘¦ via a sequence of tests.

> é€šè¿‡ç®€å•çš„å‚ç›´å’Œæ°´å¹³åˆ†å‰²

**Greedy Divide-and-conquer Strategy**

- Approach: Greedy divide-and-conquer strategy-heuristic search.

  - Start from empty tree.
  - Decide the **best feature** based on heuristics. 
  - Divide the problem into smaller subproblems;
  - Repeat (2)âˆ¼(3) until stopping criteria.

  > Heuristics: Pick the feature that maximizes information gain (ä¿¡æ¯å¢ç›Š).

 How to measure the goodness of a feature formally?

- Information gain

**Preliminary: Entropy(ç†µ)**

-  Entropy: $\mathcal{H}(Y)= -\sum_kp(y_k)\log_2p(y_k)$
- Larger entropy, more uncertainty. 
  - High entropy: ğ‘Œ âˆ¼ uniform or flat distribution â†’ less predictable
  - Low entropy: ğ‘Œ âˆ¼ peak/valley distribution â†’ more predictable

**Preliminary: Conditional Entropy**

â€¢ Conditional entropy: 
$$
\mathcal{H}(Y|X)=\sum_jp(X=x_j)H(Y|X=x_j)
$$
<img src="AAIå¤ä¹ _image/image-20250101205406751.png" alt="image-20250101205406751" style="zoom:60%;" />

<img src="AAIå¤ä¹ _image/image-20250101204902215.png" alt="image-20250101204902215" style="zoom:40%;" />



#### Information Gainï¼ˆä¿¡æ¯å¢ç›Šï¼‰

Information gain: Decrease in entropy after splitting
$$
IG(X)=\mathcal{H}(Y)-\mathcal{H}(Y|X)
$$

- ğ‘‹: input feature,
- ğ‘Œ: classification label.

<img src="AAIå¤ä¹ _image/image-20250101205156269.png" alt="image-20250101205156269" style="zoom:50%;" />

â€‹	 ğ¼ğº ğ‘ƒğ‘ğ‘¡ğ‘Ÿğ‘œğ‘›ğ‘  > ğ¼ğº ğ‘‡ğ‘¦ğ‘ğ‘’ â‡’ Patrons is better.

- When the x is **continuous**

<img src="AAIå¤ä¹ _image/image-20250101210101990.png" alt="image-20250101210101990" style="zoom:45%;" />

<img src="AAIå¤ä¹ _image/image-20250101210122396.png" alt="image-20250101210122396" style="zoom:50%;" />

#### Tree Overfitting

- More #feature, more likely overfitting; more #(train data), less likely overfitting. 
- Decision tree pruning: 
  -  Build a fully grown tree.
  - Choose a node that has only leaf nodes as children.
  - Testing the feature â€˜relevanceâ€™ for this node:
    - relevantâ†’reserve this node.
    - irrelevant: replace it based on its leaf nodes.

#### Decision Tree for Regression

- Leaf node: Linear regression model on the examples in each leaf node.

### Neural Network

<img src="AAIå¤ä¹ _image/image-20250101211553477.png" alt="image-20250101211553477" style="zoom:40%;" />

> $w_{i,j}\rightarrow w_{k,j}$ï¼Œ ä¸­é—´è¿˜è¦åŒ…æ‹¬ä¸€ä¸ªéçº¿æ€§å±‚ï¼ŒSigmodï¼ŒRelu

### K-Nearest Neighbor

 ğ‘˜-Nearest neighbor method:

- For classification: find ğ‘˜ nearest neighbors of the testing point and take a vote.

  > vote: æ ¹æ®è¿™ k ä¸ªæœ€è¿‘é‚»ä¸­å„ä¸ªç±»åˆ«çš„æ•°é‡å¤šå°‘æ¥è¿›è¡ŒæŠ•ç¥¨å†³ç­–ã€‚

- For regression: take mean or median of the ğ‘˜ nearest neighbors, or do a local regression on them.

Distance metric: $\mathscr{l}_p(x_j, x_q)=(\sum_i|s_{j,i}-x_{q,i}|^p)^{\frac{1}{p}}$

- Advantage:
  - Training is very fast. 
  - Learn complex target functions. 
  - Do not lose information.

- Disadvantage:
  - Slow at query time.
  - Easily fooled by irrelevant attributes.

#### Exercises

1. Could you classify iris with multi-class linear regression classifier?

   é€‰å–irisçš„ç‰¹å¾ï¼Œæ„é€ `MLR`modelï¼ŒLoss Functioné€‰å–MSEï¼Œè¿­ä»£è®­ç»ƒ

2. Could you classify iris with multi-class SVM classifier?

<img src="AAIå¤ä¹ _image/image-20250101213203367.png" alt="image-20250101213203367" style="zoom:50%;" />

## Lecture8 Ensemble Learning

### What is an Ensemble?

An ensemble indicates a collection of individual learning machines.

> å®ƒç»“åˆå¤šä¸ªå­¦ä¹ å™¨æ¥è§£å†³ä¸€ä¸ªé—®é¢˜ï¼Œé€šå¸¸èƒ½è·å¾—æ¯”å•ä¸ªå­¦ä¹ å™¨æ›´å¥½çš„æ€§èƒ½

Given ğ‘² base learners, whose outputs are ğ’ğ’‹ , ğ’‹ = ğŸ, ğŸ, â€¦ , ğ‘², a simple ensemble output could be
$$
O = \sum_{j=1}^K w_jo_j
$$
<img src="AAIå¤ä¹ _image/image-20250102180741314.png" alt="image-20250102180741314" style="zoom:50%;" />

**When is an Ensemble Better?**

- The errors of base learners should be independent of each other.

- The base learners should do better than random guessing (i.e., with ğœº < ğŸ. ğŸ“).

**BUT**, True independence of errors from different base learners is hard to achieve because of, e.g.

- [Possible Solutions], deal with the problem of independent base learners.
  - Use different learners to reduce the positive correlation between their errors. 
  - Different supervised learning methods.
  - Different parameters and weights.
  - Different base learners.
- Instead of pursuing mutual independence of errors of base learners, we can go one step further and encourage negative correlation of errors of base learners.

### Negative Correlation Learning

Instead of creating an ensemble of unbiased individual networks whose errors are uncorrelated, NCL can produce individual networks whose errors are negatively correlated.

> é€šè¿‡å¼•å…¥è´Ÿç›¸å…³æœºåˆ¶æ¥æ”¹è¿›æ¨¡å‹çš„æ€§èƒ½

### Other Methods for Constructing an Ensemeble Classifier

#### Bagging

Bootstrap Aggregatingï¼Œå¸¸ç®€ç§°ä¸º Baggingï¼Œæ˜¯ä¸€ç§é›†æˆå­¦ä¹ ï¼ˆEnsemble Learningï¼‰æ–¹æ³•ã€‚å®ƒçš„ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œæœ‰æ”¾å›çš„æŠ½æ ·ï¼ˆè¿™ç§æŠ½æ ·æ–¹æ³•è¢«ç§°ä¸º Bootstrap æŠ½æ ·ï¼‰ï¼Œç”Ÿæˆå¤šä¸ªä¸åŒçš„è®­ç»ƒå­é›†ï¼Œç„¶åä½¿ç”¨è¿™äº›å­é›†åˆ†åˆ«è®­ç»ƒå¤šä¸ªåŸºå­¦ä¹ å™¨ï¼ˆBase learnerï¼‰ï¼Œæœ€åå°†è¿™äº›åŸºå­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœè¿›è¡Œç»¼åˆï¼ˆé€šå¸¸æ˜¯ç®€å•å¹³å‡æˆ–å¤šæ•°æŠ•ç¥¨ï¼‰æ¥å¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

- Improves the generalization error by reducing the variance of the base classifiers.

  >  é€šè¿‡å‡å°‘åŸºåˆ†ç±»å™¨çš„æ–¹å·®ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›

#### Boosting

å°†å¤šä¸ªå¼±åˆ†ç±»å™¨ç»„åˆæˆå¼ºåˆ†ç±»å™¨ï¼Œé€šè¿‡è¿­ä»£è‡ªé€‚åº”æ”¹å˜è®­ç»ƒæ ·æœ¬åˆ†å¸ƒï¼Œä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬åˆ†é…æƒé‡ï¼Œæ¯æ¬¡è¿­ä»£å¢åŠ é”™è¯¯åˆ†ç±»æ ·æœ¬æƒé‡ã€é™ä½æ­£ç¡®åˆ†ç±»æ ·æœ¬æƒé‡å¹¶å½’ä¸€åŒ–ï¼Œæœ€åèšåˆè®­ç»ƒå¥½çš„åŸºåˆ†ç±»å™¨ä½œä¸ºæœ€ç»ˆé›†æˆæ¨¡å‹ã€‚

- Two core components:
  - Weights:
    - Used as a sampling distribution when creating bootstraps.
    - Used by the base classifier to learn a model which is biased towards the examples that are hard to classify (ones with higher weights).

- Final ensemble = weighted-majority/weighted-average combination

#### AdaBoost

è¾“å…¥å¸¦æ ‡ç­¾æ ·æœ¬é›†ï¼Œåˆå§‹åŒ–æ ·æœ¬æƒé‡ä¸º$\frac{1}{N}$ï¼Œæ¯æ¬¡è¿­ä»£æ ¹æ®æƒé‡é‡‡æ ·ç”Ÿæˆè‡ªåŠ©æ ·æœ¬ï¼Œè®­ç»ƒåŸºæ¨¡å‹ï¼Œç»Ÿè®¡é”™è¯¯åˆ†ç±»æ ·æœ¬å¹¶æ›´æ–°æƒé‡ï¼ˆé”™è¯¯åˆ†ç±»æ ·æœ¬æƒé‡ä¹˜ä»¥ï¼‰ï¼Œæœ€åå½’ä¸€åŒ–æƒé‡ï¼Œè®¡ç®—åŸºæ¨¡å‹æƒé‡ï¼Œé€šè¿‡åŠ æƒå¤šæ•°æŠ•ç¥¨ç»„åˆåŸºæ¨¡å‹ã€‚

#### Random Forest

ç”±å¤šä¸ªå†³ç­–æ ‘ç»„æˆï¼Œæ¯ä¸ªæ ‘åŸºäºç‹¬ç«‹éšæœºå‘é‡ç”Ÿæˆï¼Œéšæœºå‘é‡ä»å›ºå®šæ¦‚ç‡åˆ†å¸ƒç”Ÿæˆï¼ˆä¸ AdaBoost çš„è‡ªé€‚åº”æ–¹æ³•ä¸åŒï¼‰ï¼ŒBagging æ˜¯å†³ç­–æ ‘çš„ä¸€ä¸ªç‰¹æ®Šæƒ…å†µã€‚

> Many decision tree, More stable, better generalization

- Bagging using DTs is a special case of random forest.

<img src="AAIå¤ä¹ _image/image-20250101231332331.png" alt="image-20250101231332331" style="zoom:50%;" />

#### Current Status

- å›å½’é—®é¢˜ä¸­ï¼Œåœ¨ä¸€å®šå‡è®¾ä¸‹é›†æˆæ€§èƒ½ä¸åŠ£äºå•ä¸ªå­¦ä¹ å™¨ï¼›åˆ†ç±»é—®é¢˜ä¸­ï¼Œè™½æ— ä¸¥æ ¼è¯æ˜ï¼Œä½†æœ‰å¤§é‡ç»éªŒè¯æ®è¡¨æ˜é›†æˆä¼˜äºå•ä¸ªå­¦ä¹ å™¨ã€‚
- åˆ†ç±»é—®é¢˜ä¸­ï¼Œè™½æ— ä¸¥æ ¼è¯æ˜ï¼Œä½†æœ‰å¤§é‡ç»éªŒè¯æ®è¡¨æ˜é›†æˆä¼˜äºå•ä¸ªå­¦ä¹ å™¨ã€‚

<img src="AAIå¤ä¹ _image/image-20250101231808549.png" alt="image-20250101231808549" style="zoom:50%;" />

#### Simple Ensemble Approaches

- Train different models on the same dataset, then vote (classification) or average (regression) of predictions of multiple trained models

  > ä¸åŒçš„modelï¼Œç›¸åŒçš„Dataset

- Train the same model multiple times on different data sets generated from the original data set.

  > ç›¸åŒçš„modelï¼Œä¸åŒçš„dataset

- Train different models on multiple datasets.

  > model å’Œ datasetséƒ½ä¸ä¸€æ ·

<img src="AAIå¤ä¹ _image/image-20250101232039331.png" alt="image-20250101232039331" style="zoom:50%;" />

- Majority voting
- Weighted voting
- Averaging
- Weighted averaging

<img src="AAIå¤ä¹ _image/image-20250101232131244.png" alt="image-20250101232131244" style="zoom:50%;" />

### Lab

**stacking algorithm**

Stacking ç®—æ³•ï¼Œå³å †å æ³›åŒ–ç®—æ³•ï¼ˆStacked Generalizationï¼‰ï¼Œæ˜¯ä¸€ç§ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹èåˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ç»„åˆå¤šä¸ªåŸºæ¨¡å‹çš„é¢„æµ‹ç»“æœæ¥åˆ›å»ºä¸€ä¸ªæ›´å¼ºå¤§çš„å…ƒæ¨¡å‹ï¼Œä»¥æå‡æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚

## Lecture9 Multi-Objective Optimization and Learning

**Pareto Front**

ç”±æ‰€æœ‰éæ”¯é…è§£ï¼ˆåœ¨ç›®æ ‡ç©ºé—´ä¸­ä¸è¢«å…¶ä»–è§£å¸•é›·æ‰˜ä¼˜äºçš„è§£ï¼‰æ„æˆçš„é›†åˆï¼Œå†³ç­–ç©ºé—´ä¸­çš„å¯¹åº”é›†åˆä¸ºå¸•é›·æ‰˜æœ€ä¼˜é›†ã€‚

å¸•ç´¯æ‰˜å‰æ²¿ä»£è¡¨äº†è¿™æ ·ä¸€ç»„è§£çš„é›†åˆï¼šåœ¨è¿™ä¸ªé›†åˆä¸­çš„è§£ï¼Œåœ¨ä¸ä½¿å…¶ä»–ç›®æ ‡å˜å·®çš„æƒ…å†µä¸‹ï¼Œæ— æ³•å†è¿›ä¸€æ­¥ä¼˜åŒ–ä»»ä½•ä¸€ä¸ªç›®æ ‡äº†ï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰ç›®æ ‡ä¹‹é—´è¾¾åˆ°äº†ä¸€ç§æƒè¡¡æœ€ä¼˜çš„çŠ¶æ€ã€‚

**Main Goal**

- **æ”¶æ•›æ€§ï¼ˆConvergenceï¼‰**ï¼šæ‰¾åˆ°å°½å¯èƒ½æ¥è¿‘å¸•é›·æ‰˜æœ€ä¼˜å‰æ²¿çš„ä¸€ç»„è§£ã€‚
- **å¤šæ ·æ€§ï¼ˆDiversityï¼‰**ï¼šæ‰¾åˆ°å°½å¯èƒ½å¤šæ ·çš„ä¸€ç»„è§£ã€‚

### MOEAs

**æ‹¥æŒ¤è·ç¦»ï¼ˆCrowding Distanceï¼‰**ï¼šè¡¨ç¤ºåŒä¸€ç­‰çº§ä¸­åŒ…å›´ç‰¹å®šè§£çš„æœ€è¿‘é‚»åŸŸè§£çš„**é•¿æ–¹ä½“å‘¨é•¿çš„ä¸€åŠ**ï¼Œç”¨äºå¯†åº¦ä¼°è®¡ã€‚

#### NSGA-II

- ç®—æ³•æ­¥éª¤
  - åˆå¹¶çˆ¶ä»£å’Œå­ä»£ç§ç¾¤ï¼Œé€‰å–å‰æ²¿å¡«å……çˆ¶ä»£ç§ç¾¤ã€‚
  - å¯¹ç§ç¾¤æŒ‰éæ”¯é…æ’åºå¹¶é€‰æ‹©å‰ä¸ªå…ƒç´ ã€‚
  - ä½¿ç”¨é€‰æ‹©ã€äº¤å‰å’Œå˜å¼‚åˆ›å»ºæ–°ç§ç¾¤ã€‚
- ä¼˜ç¼ºç‚¹
  - **ä¼˜ç‚¹**ï¼šé€šè¿‡æ‹¥æŒ¤è¿‡ç¨‹ä¿æŒéæ”¯é…è§£çš„å¤šæ ·æ€§ï¼Œæ— éœ€é¢å¤–å¤šæ ·æ€§æ§åˆ¶ï¼›ç²¾è‹±ä¸»ä¹‰ä¿æŠ¤å·²æ‰¾åˆ°çš„å¸•é›·æ‰˜æœ€ä¼˜è§£ä¸è¢«åˆ é™¤ã€‚
  - **ç¼ºç‚¹**ï¼šå½“ç¬¬ä¸€ä¸ªéæ”¯é…é›†æˆå‘˜è¶…è¿‡Nä¸ªæ—¶ï¼Œå¯èƒ½ä¸¢å¼ƒä¸€äº›å¸•é›·æ‰˜æœ€ä¼˜è§£ã€‚

### Multi-Objective Learning

Multi-task learning: in short, multi-task learning is defined as learning multiple objective functions loss at the same time.

<img src="AAIå¤ä¹ _image/image-20250101235454817.png" alt="image-20250101235454817" style="zoom:50%;" />

<img src="AAIå¤ä¹ _image/image-20250101235909165.png" alt="image-20250101235909165" style="zoom:80%;" />

> hard parameteså…±äº«æ˜¯æŒ‡å¤šä¸ªä»»åŠ¡ä¹‹é—´å…±äº«å…¨éƒ¨æˆ–è€…éƒ¨åˆ†æ¨¡å‹å‚æ•°ã€‚è¿™ç§æ–¹å¼èƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¨¡å‹çš„å‚æ•°æ€»é‡ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚å¦‚æœä¸åŒä»»åŠ¡ä¹‹é—´çš„å·®å¼‚è¾ƒå¤§ï¼Œå…±äº«çš„å‚æ•°å¯èƒ½æ— æ³•å¾ˆå¥½åœ°é€‚åº”æ‰€æœ‰ä»»åŠ¡çš„éœ€æ±‚ï¼Œå¯¼è‡´ä»»åŠ¡ä¹‹é—´äº§ç”Ÿå¹²æ‰°ï¼Œå½±å“æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚
>
> soft parameterså…±äº«å¹¶ä¸ç›´æ¥å…±äº«æ¨¡å‹çš„å‚æ•°ï¼Œè€Œæ˜¯é€šè¿‡æŸç§æœºåˆ¶ä½¿ä¸åŒä»»åŠ¡çš„å‚æ•°ä¹‹é—´ç›¸äº’å½±å“ï¼Œä»è€Œè¾¾åˆ°éšå¼å…±äº«ç‰¹å¾çš„ç›®çš„ã€‚è½¯å‚æ•°å…±äº«èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒä»»åŠ¡ä¹‹é—´çš„å·®å¼‚ï¼Œæ¯ä¸ªä»»åŠ¡å¯ä»¥æ ¹æ®è‡ªèº«çš„ç‰¹ç‚¹æ¥è°ƒæ•´å‚æ•°ï¼ŒåŒæ—¶åˆèƒ½ä»å…¶ä»–ä»»åŠ¡ä¸­å­¦ä¹ åˆ°ä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯ã€‚ç”±äºæ¯ä¸ªä»»åŠ¡éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„å‚æ•°ï¼Œæ¨¡å‹çš„å‚æ•°æ€»é‡ç›¸å¯¹è¾ƒå¤§ï¼Œè®­ç»ƒæ—¶é—´å’Œè®¡ç®—èµ„æºçš„æ¶ˆè€—ä¹Ÿä¼šå¢åŠ ã€‚

## Lecture10 UnsupervisedLearning

Clustering  is an unsupervised learning methods, because the labels are not given.

èšç±»æ˜¯å°†ç›¸ä¼¼å¯¹è±¡æˆ–æ•°æ®åˆ†ç»„åœ¨ä¸€èµ·çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å‘ç°æ•°æ®ä¸­çš„éšå«æ¨¡å¼ã€å±æ€§å’Œç»“æ„ã€‚å…¶å®šä¹‰ä¸ºç»™å®š n ä¸ªå¯¹è±¡çš„è¡¨ç¤ºï¼ŒåŸºäºç›¸ä¼¼æ€§åº¦é‡æ‰¾åˆ° k ä¸ªç»„ï¼Œä½¿åŒä¸€ç»„å†…å¯¹è±¡ç›¸ä¼¼åº¦é«˜ï¼Œä¸åŒç»„é—´ç›¸ä¼¼åº¦ä½ã€‚

MInkowski distance($L_P$-distance)
$$
d(x_i,x_j) = (\sum_{l=1}^N|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}
$$
Chebyshev distance($L_{\infin}$-distance)
$$
d(x_i,x_j)=\max_{1\leq l \leq n}|x_i^{(l)}-x_j^{(l)}|
$$

### Hierarchical Algorithm

å±‚æ¬¡èšç±»ç®—æ³•ä»¥é€’å½’æ–¹å¼å¯»æ‰¾åµŒå¥—çš„ç°‡ï¼Œè¦ä¹ˆé‡‡ç”¨å‡èšæ¨¡å¼ï¼ˆbottom-up, ä»æ¯ä¸ªæ•°æ®ç‚¹è‡ªæˆä¸€ä¸ªç°‡å¼€å§‹ï¼Œä¾æ¬¡åˆå¹¶æœ€ç›¸ä¼¼çš„ä¸€å¯¹ç°‡ä»¥å½¢æˆç°‡å±‚æ¬¡ç»“æ„ï¼‰ï¼›è¦ä¹ˆé‡‡ç”¨åˆ†è£‚ï¼ˆtop-down, è‡ªä¸Šè€Œä¸‹ï¼‰æ¨¡å¼ï¼ˆä»æ‰€æœ‰æ•°æ®ç‚¹åœ¨ä¸€ä¸ªç°‡å¼€å§‹ï¼Œé€’å½’åœ°å°†æ¯ä¸ªç°‡åˆ’åˆ†ä¸ºæ›´å°çš„ç°‡ï¼‰ã€‚

#### Question

How do you represent a cluster of more than one point?

æ¬§å‡ é‡Œå¾—ç©ºé—´ç”¨è´¨å¿ƒï¼ˆcentroidï¼‰è¡¨ç¤ºï¼Œå³æ•°æ®ç‚¹å¹³å‡å€¼ï¼›éæ¬§å‡ é‡Œå¾—ç©ºé—´ç”¨èšç±»ç‚¹ï¼ˆclustroidï¼‰ï¼Œå³ç¦»å…¶ä»–ç‚¹ â€œæœ€è¿‘â€ çš„ç‚¹ï¼Œå…¶ â€œæœ€è¿‘â€ å®šä¹‰æœ‰å¤šç§ï¼Œå¦‚æœ€å°æœ€å¤§è·ç¦»ã€æœ€å°å¹³å‡è·ç¦»ã€æœ€å°è·ç¦»å¹³æ–¹å’Œç­‰ã€‚

How do you determine the â€œnearness/similarityâ€ of clusters?

> Measure cluster distances by distances of centroids

å¯å°†èšç±»ç‚¹è§†ä¸ºè´¨å¿ƒè®¡ç®—ç°‡é—´è·ç¦»ï¼›æˆ–å–ä¸¤ç°‡é—´ä»»æ„ä¸¤ç‚¹è·ç¦»æœ€å°å€¼ï¼›æˆ–å®šä¹‰ â€œå‡èšåº¦â€ æ¦‚å¿µï¼Œå¦‚æœ€å¤§è·ç¦»ï¼Œåˆå¹¶æœ€å‡èšçš„ç°‡ã€‚å‡èšåº¦å¯é€šè¿‡ç°‡ç›´å¾„ï¼ˆæœ€å¤§ç‚¹é—´è·ç¦»ï¼‰ã€å¹³å‡è·ç¦»æˆ–å¯†åº¦ç›¸å…³æ–¹æ³•è¡¡é‡ã€‚

When to stop combining clusters?

- åŸºäºç¨³å®šæ€§

  è§‚å¯Ÿåˆå¹¶è¿‡ç¨‹ä¸­æŸäº›ç¨³å®šæ€§æŒ‡æ ‡çš„å˜åŒ–ï¼Œä¾‹å¦‚ç°‡å†…æ–¹å·®ã€ç°‡é—´è·ç¦»çš„å˜åŒ–ç‡ç­‰ã€‚å¦‚æœåœ¨è¿ç»­å¤šæ¬¡åˆå¹¶åï¼Œè¿™äº›æŒ‡æ ‡çš„å˜åŒ–å¾ˆå°æˆ–è€…ä¸å†æœ‰æ˜æ˜¾å˜åŒ–ï¼Œè®¤ä¸ºèšç±»ç»“æ„å·²ç»ç›¸å¯¹ç¨³å®šï¼Œæ­¤æ—¶åœæ­¢åˆå¹¶ã€‚

- åŸºäºè·ç¦»é˜ˆå€¼

  è®¾å®šä¸€ä¸ªè·ç¦»é˜ˆå€¼ï¼Œå½“è¦åˆå¹¶çš„ä¸¤ä¸ªç°‡ä¹‹é—´çš„è·ç¦»ï¼ˆæ ¹æ®æ‰€é‡‡ç”¨çš„è·ç¦»åº¦é‡æ–¹å¼ï¼Œå¦‚è´¨å¿ƒè·ç¦»ã€æœ€å°è·ç¦»ã€æœ€å¤§è·ç¦»ç­‰ï¼‰è¶…è¿‡è¿™ä¸ªé˜ˆå€¼æ—¶ï¼Œåœæ­¢åˆå¹¶

#### The Non-Euclidean Case

éæ¬§å‡ é‡Œå¾—ç©ºé—´ç”¨èšç±»ç‚¹ï¼ˆclustroidï¼‰ï¼Œå³ç¦»å…¶ä»–ç‚¹ â€œæœ€è¿‘â€ çš„ç‚¹ï¼Œå…¶ â€œæœ€è¿‘â€ å®šä¹‰æœ‰å¤šç§ï¼Œå¦‚:

- æœ€å°æœ€å¤§è·ç¦», 
- æœ€å°å¹³å‡è·ç¦»,
- æœ€å°è·ç¦»å¹³æ–¹å’Œç­‰ã€‚

**å®ç°ä¸å¤æ‚åº¦**ï¼šæœ´ç´ å®ç°æ¯æ¬¡è®¡ç®—æ‰€æœ‰ç°‡å¯¹è·ç¦»å†åˆå¹¶ï¼Œå¤æ‚åº¦ä¸º O (NÂ³)ï¼›ç²¾å¿ƒå®ç°ä½¿ç”¨ä¼˜å…ˆé˜Ÿåˆ—å¯é™è‡³ O (NÂ² log (N))ï¼Œä½†å¯¹äºå¤§æ•°æ®é›†ä»è¾ƒæ˜‚è´µã€‚

#### Well-known Hierarchical Algorithms

- **å•é“¾èšç±»ï¼ˆSingle-link Clusteringï¼‰**ï¼šä¸¤ç°‡ç›¸ä¼¼åº¦ä¸ºæœ€ç›¸ä¼¼æˆå‘˜é—´ç›¸ä¼¼åº¦ï¼Œå…³æ³¨ç°‡æœ€æ¥è¿‘åŒºåŸŸï¼Œå…· â€œä¹è§‚â€ æ€§ï¼Œæ­¥éª¤åŒ…æ‹¬åˆå§‹åŒ–ã€æ‰¾æœ€ç›¸ä¼¼ç°‡å¯¹ã€åˆå¹¶ã€æ›´æ–°çŸ©é˜µã€é‡å¤ç›´è‡³åªå‰©ä¸€ä¸ªç°‡ã€‚
- **å…¨é“¾èšç±»ï¼ˆComplete-link Clusteringï¼‰**ï¼šä¸¤ç°‡ç›¸ä¼¼åº¦ä¸ºæœ€ä¸ç›¸ä¼¼æˆå‘˜é—´ç›¸ä¼¼åº¦ï¼Œè€ƒè™‘èšç±»æ•´ä½“ç»“æ„ï¼Œå…· â€œæ‚²è§‚â€ æ€§ï¼Œæ­¥éª¤ä¸å•é“¾èšç±»ç±»ä¼¼ï¼Œä½†æ›´æ–°çŸ©é˜µæ—¶è®¡ç®—æ–°ç°‡ä¸æ—§ç°‡è·ç¦»æ–¹å¼ä¸åŒã€‚

### k-means and Other Cost Minimization Clustering

#### k - å‡å€¼èšç±»ï¼ˆk - means Clusteringï¼‰

- **ç›®æ ‡å‡½æ•°**ï¼šåœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­ï¼Œæœ€å°åŒ–æ‰€æœ‰ç‚¹åˆ°å…¶æ‰€å±ç°‡è´¨å¿ƒçš„å¹³æ–¹è·ç¦»ä¹‹å’Œï¼Œæ˜¯ NP éš¾é—®é¢˜ï¼Œè´ªå¿ƒç®—æ³•æ”¶æ•›äºå±€éƒ¨æœ€ä¼˜ã€‚

- **ä¸»è¦è¿‡ç¨‹**ï¼šé€‰æ‹©åˆå§‹ k ä¸ªç°‡ï¼ˆå¦‚éšæœºé€‰ä¸€ä¸ªç‚¹ï¼Œå†é€‰ k-1 ä¸ªæœ€è¿œç‚¹ä¸ºè´¨å¿ƒï¼Œå°†å„ç‚¹åˆ†é…åˆ°æœ€è¿‘è´¨å¿ƒç°‡ï¼Œæ›´æ–°è´¨å¿ƒï¼Œé‡å¤ç›´è‡³ç°‡æˆå‘˜ç¨³å®šï¼‰ã€‚

- **é—®é¢˜**ï¼š

  - åˆå§‹åŒ–å½±å“ç»“æœï¼›éœ€é€‰æ‹©åˆé€‚è·ç¦»åº¦é‡ï¼›

  - ç°‡æ•°é‡ k é€‰æ‹©ç¼ºä¹ç†è®ºæ”¯æŒï¼Œå¯é€šè¿‡è§‚å¯Ÿå¹³å‡è·ç¦»éš k å˜åŒ–æ¥é€‰æ‹©ï¼Œè·ç¦»ä¸‹é™å¿«åˆ°åˆé€‚ k åå˜åŒ–å°ï¼Œk è¿‡å¤§å¹³å‡è·ç¦»æ”¹è¿›å°ã€‚

  - Average falls rapidly until right ğ’Œ, then changes little.

    > èšç±»ä¸­å¿ƒçš„å‡å€¼ä¸‹é™å¾ˆå¤§ç„¶åä¸‹é™å¾ˆå°çš„é‚£ä¸ªè½¬æŠ˜ç‚¹å°±æ˜¯æœ€ä½³çš„k

#### Spectral Clustering

å°†æ•°æ®ç‚¹è¡¨ç¤ºä¸ºåŠ æƒå›¾èŠ‚ç‚¹ï¼Œè¾¹æƒé‡ä¸ºç‚¹å¯¹ç›¸ä¼¼åº¦ï¼Œç›®æ ‡æ˜¯å°†èŠ‚ç‚¹åˆ’åˆ†ä¸ºå­é›†ä½¿å‰²å¤§å°ï¼ˆè¿æ¥ä¸åŒå­é›†èŠ‚ç‚¹è¾¹æƒé‡å’Œï¼‰æœ€å°ï¼Œä½†æœ€å°å‰²ç®—æ³•å¸¸å¯¼è‡´ä¸å¹³è¡¡ç°‡ã€‚

## Lecture11 Feature Engineering

### Feature Engineering

ç‰¹å¾æ˜¯æè¿°é—®é¢˜ä¸”å¯¹é¢„æµ‹æˆ–è§£å†³é—®é¢˜æœ‰ç”¨çš„ä¿¡æ¯ã€‚ç‰¹å¾å·¥ç¨‹æ˜¯ç¡®å®šå“ªäº›ç‰¹å¾å¯¹è®­ç»ƒæ¨¡å‹æœ‰ç”¨ï¼Œç„¶åé€šè¿‡è½¬æ¢åŸå§‹æ•°æ®æ¥åˆ›å»ºè¿™äº›ç‰¹å¾çš„è¿‡ç¨‹ã€‚

**Why Normalize Numeric Features?**

å½“åŒä¸€ç‰¹å¾å€¼å·®å¼‚å¤§æˆ–ä¸åŒç‰¹å¾èŒƒå›´å·®å¼‚å¤§æ—¶ï¼Œå½’ä¸€åŒ–å¯é¿å…è®­ç»ƒé—®é¢˜ï¼Œå¦‚æ¢¯åº¦æ›´æ–°è¿‡å¤§å¯¼è‡´è®­ç»ƒå¤±è´¥æˆ–æ¢¯åº¦ä¸‹é™ â€œåå¼¹â€ å½±å“æ”¶æ•›ï¼Œå¯é‡‡ç”¨å¼‚è´¨å­¦ä¹ ç‡è§£å†³ã€‚

![image-20250102130802509](AAIå¤ä¹ _image/image-20250102130802509.png)

**åˆ†ç±»æ•°æ®è½¬æ¢**

- One/Multi-hot encoding
- Hashing
- Embeddings: high-dimensional --> low-dimensional space

**Three Typical Methods**

- **åŒ…è£…å™¨æ–¹æ³•ï¼ˆWrapper methodsï¼‰**ï¼šæ¨¡å‹ç›¸å…³ï¼Œé€šè¿‡æ·»åŠ  / åˆ é™¤ç‰¹å¾å¯¼èˆªç‰¹å¾å­é›†ï¼Œåœ¨éªŒè¯é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œé‡å¤ç›´åˆ°ç²¾åº¦æ— æé«˜ï¼Œä¼˜ç‚¹æ˜¯å‡†ç¡®æ€§é«˜ï¼Œç¼ºç‚¹æ˜¯è®¡ç®—æ˜‚è´µä¸”æœ‰è¿‡æ‹Ÿåˆé£é™©ã€‚

- **è¿‡æ»¤å™¨æ–¹æ³•ï¼ˆFilter methodsï¼‰**ï¼šç‹¬ç«‹äºå­¦ä¹ æ¨¡å‹ï¼Œæ ¹æ®ä¸ AI ä»»åŠ¡ç›¸å…³æ€§çš„å¯å‘å¼åˆ†æ•°å¯¹ç‰¹å¾æ’åå¹¶é€‰æ‹©å­é›†ï¼Œä¼˜ç‚¹æ˜¯å¿«é€Ÿç®€å•ã€æ³›åŒ–æ€§å¥½ï¼Œç¼ºç‚¹æ˜¯æœªè€ƒè™‘ç‰¹å¾é—´ç›¸äº’ä½œç”¨ã€å‡†ç¡®æ€§ä¸å¦‚åŒ…è£…å™¨æ–¹æ³•ï¼Œå¯ä½œä¸ºåŒ…è£…å™¨ç‰¹å¾é€‰æ‹©çš„é¢„å¤„ç†ï¼Œå¦‚åŸºäºç›¸å…³æ€§çš„è¿‡æ»¤å™¨ï¼ˆå‡è®¾å¥½ç‰¹å¾ä¸è¾“å‡ºé«˜åº¦ç›¸å…³ä¸”ç›¸äº’é—´ä¸é«˜åº¦ç›¸å…³ï¼Œé€šè¿‡ç›¸å…³æ€§åˆ†æ•°è¿›è¡Œç‰¹å¾æ’åã€å»é™¤å†—ä½™ç›¸å…³ç‰¹å¾ï¼‰ï¼Œç›¸å…³æ€§åº¦é‡åŒ…æ‹¬ç»å…¸çº¿æ€§ç›¸å…³ï¼ˆå¦‚ Pearson ç›¸å…³ï¼Œè®¡ç®—ç®€å•ä½†ä¸èƒ½æ•è·éçº¿æ€§ç›¸å…³ä¸”è¦æ±‚æ•°å€¼ç‰¹å¾ï¼‰å’Œä¿¡æ¯è®ºæ–¹æ³•ï¼ˆå¦‚ä¿¡æ¯å¢ç›Šï¼Œèƒ½æ•è·éçº¿æ€§ç›¸å…³ä½†è®¡ç®—æˆæœ¬é«˜ä¸”åå‘å€¼å¤šçš„ç‰¹å¾ï¼›å¯¹ç§°ä¸ç¡®å®šæ€§ï¼Œå¯è¡¥å¿ä¿¡æ¯å¢ç›Šåå·®å¹¶å½’ä¸€åŒ–å€¼åˆ° [0,1]ï¼‰ã€‚

- **åµŒå…¥å¼æ–¹æ³•ï¼ˆEmbedded methodsï¼‰**ï¼šç‰¹å¾é€‰æ‹©æ˜¯æ¨¡å‹æ„å»ºä¸€éƒ¨åˆ†ï¼Œç”±å­¦ä¹ è¿‡ç¨‹å¼•å¯¼ç‰¹å¾æœç´¢ï¼Œåˆ©ç”¨ç®—æ³•è¿”å›æ¨¡å‹ç»“æ„è·å–ç›¸å…³ç‰¹å¾é›†ï¼Œä¼˜ç‚¹æ˜¯ç±»ä¼¼åŒ…è£…å™¨æ–¹æ³•ä½†è®¡ç®—æˆæœ¬ä½ä¸”ä¸æ˜“è¿‡æ‹Ÿåˆï¼Œå¦‚å†³ç­–æ ‘ï¼ˆæ„å»ºè¿‡ç¨‹å°±æ˜¯ç‰¹å¾é€‰æ‹©è¿‡ç¨‹ï¼Œæ ‘ä¸­æœªä½¿ç”¨æ‰€æœ‰ç‰¹å¾ï¼‰ã€‚

  > åµŒå…¥åˆ°æ¨¡å‹ä¸­ï¼Œå’Œæ¨¡å‹ä¸€èµ·è®­ç»ƒ

### Regularization

**æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰**

- **åŸºæœ¬æ€æƒ³**ï¼šæ¨¡å‹ä¸­ç‰¹å¾è¶Šå¤šå¤æ‚åº¦è¶Šå¤§ï¼Œæ­£åˆ™åŒ–é€šè¿‡å¯¹å¤æ‚åº¦å¼•å…¥æƒ©ç½šæ¥å‡å°‘ç‰¹å¾ï¼Œä½¿æ¨¡å‹å€¾å‘äºä½å¤æ‚åº¦ï¼ˆå°‘ç‰¹å¾ï¼‰ï¼Œä»è´å¶æ–¯è§‚ç‚¹çœ‹æ˜¯ç»™å­¦ä¹ æ¨¡å‹æ–½åŠ ä¸–ç•Œæ˜¯ç®€å•çš„å…ˆéªŒçŸ¥è¯†ã€‚

H<img src="AAIå¤ä¹ _image/image-20250102132841845.png" alt="image-20250102132841845" style="zoom:50%;" />

- Ridge regression
  $$
  J = \frac{1}{n}\sum_{i=1}^n(f(x_i)-y_i)^2+\lambda||w||_2^2
  $$
  
- Lasso regression
  $$
  J = \frac{1}{n}\sum_{i=1}^n(f(x_i)-y_i)^2+\lambda||w||_1
  $$

  > Lasso can be used for feature selection, but ridge regression cannot. In other words, Lasso is easier to make the weight become 0, while ridge is easier to make the weight close to 0.S

**Summary**

- Feature selection is a heuristic search problem. 
- Use regularization on all possible features to prevent overfitting.

## Lecture12 Markov Decision Process

å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§è®¡ç®—æ–¹æ³•ï¼Œæ™ºèƒ½ä½“ï¼ˆagentï¼‰é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œä»å¥–åŠ±ä¿¡å·ä¸­å­¦ä¹ å¦‚ä½•å°†æƒ…å¢ƒæ˜ å°„åˆ°è¡ŒåŠ¨ï¼Œä»¥æœ€å¤§åŒ–æ•°å€¼å¥–åŠ±ã€‚

<img src="AAIå¤ä¹ _image/image-20250102144036693.png" alt="image-20250102144036693" style="zoom:50%;" />

**Markov property:** Given the present, the future is independent of the history.

> ç»™å®šå½“å‰æ—¶åˆ»ï¼Œæœªæ¥ä¸å†å²çŠ¶æ€æ— å…³

<img src="AAIå¤ä¹ _image/image-20250102144815715.png" alt="image-20250102144815715" style="zoom:40%;" />

### Core Elements of RL

**Policy**

A policy defines an agentâ€™s behaviour (mapping from state to action), i.e. how the agent acts in the given circumstance.

> ç­–ç•¥å®šä¹‰æ™ºèƒ½ä½“è¡Œä¸ºï¼Œæ˜¯ä»çŠ¶æ€åˆ°è¡ŒåŠ¨çš„æ˜ å°„ã€‚

- A stochastic policy ğ… is a conditional probability distribution over actions given states
  $$
  \pi(\pi|s)=\mathbb{P}(A_t=a|S_t=s)
  $$

- Both are stationary (time-independent) and historyindependent.

**Value**

- A value function: a prediction of future reward which is used to evaluate state(s) so as to select hopefully optimal action(s).

  > åŒ…å«äº†æœªæ¥rewardçš„æœŸæœ›å€¼ï¼Œç”¨æ¥è¯„ä¼°stateçš„å¥½å

**Value Functions**

- Define <u>state</u> value function $v_{\pi}(s):S\rightarrow\mathbb{R}$ of policy $\pi$ as 
  $$
  v_{\pi}(s) = \mathbb{E}[G_t|\pi,S_t=s]
  $$

- Define <u>state</u> value function $v_{\pi}(s):S\times \mathcal{A}\rightarrow\mathbb{R}$ of policy $\pi$ as 

$$
q_{\pi}(s,a) = \mathbb{E}[G_t|\pi,S_t=s,A_t=a]
$$

<img src="AAIå¤ä¹ _image/image-20250102151157449.png" alt="image-20250102151157449" style="zoom:50%;" />

<img src="AAIå¤ä¹ _image/image-20250102151217154.png" alt="image-20250102151217154" style="zoom:50%;" />

**Bellman equation**
$$
v_{\pi}=R_{\pi}+\gamma P_{\pi}v_{\pi}
$$
<img src="AAIå¤ä¹ _image/image-20250102151411952.png" alt="image-20250102151411952" style="zoom:50%;" />

<img src="AAIå¤ä¹ _image/image-20250102151424303.png" alt="image-20250102151424303" style="zoom:50%;" />

## Lecture13 Reinforcement Learning

### Model-based RL

Model-based reinforcement learning: maintain an estimated MDP ğ‘€(â€œmodelâ€) and use it as the input of the planning algorithms.

> ç»´æŠ¤ä¸€ä¸ªä¼°è®¡çš„MDPï¼Œç”¨å®ƒæ¥ä½œä¸ºç®—æ³•çš„è¾“å…¥

<img src="AAIå¤ä¹ _image/image-20250102152900825.png" alt="image-20250102152900825" style="zoom:50%;" />

### Exploration vs Exploitation

- **æ¢ç´¢ï¼ˆExplorationï¼‰**ï¼šæ•…æ„é‡‡å–å½“å‰çŸ¥è¯†ä¸‹çœ‹ä¼¼éæœ€ä¼˜çš„è¡ŒåŠ¨ä»¥è·å–æ›´å¤šä¿¡æ¯å‘ç°æ›´å¥½ç­–ç•¥ã€‚
- **åˆ©ç”¨ï¼ˆExploitationï¼‰**ï¼šé‡‡å–å½“å‰è®¤ä¸ºæœ€ä¼˜çš„è¡ŒåŠ¨ã€‚

**$\epsilon$-Greedy**

ä»¥æ¦‚ç‡$\epsilon$é€‰æ‹©éšæœºè¡ŒåŠ¨ï¼Œä»¥æ¦‚ç‡$1-\epsilon$é€‰æ‹© â€œæœ€ä¼˜â€ è¡ŒåŠ¨ï¼Œ$\epsilon$å¸¸å–å°å€¼ã€‚ä¼˜ç‚¹æ˜¯æ˜“å®ç°ä¸”å¤šæ•°æ—¶å€™é‡‡å–æœ€ä¼˜è¡ŒåŠ¨ï¼ˆåˆå§‹ç­–ç•¥è¶³å¤Ÿå¥½æ—¶ä¸æ˜“é‡‡å–ç¾éš¾æ€§è¡ŒåŠ¨ï¼‰ï¼Œç¼ºç‚¹æ˜¯æ™ºèƒ½ä½“è¡Œä¸ºæ°¸ä¸æ”¶æ•›ï¼ˆæŒç»­æ¢ç´¢ï¼‰ï¼ŒæŸäº›æƒ…å†µä¸‹å¯èƒ½æŒ‡æ•°çº§ä½æ•ˆï¼ˆå¦‚ç¤ºä¾‹ä¸­æ™ºèƒ½ä½“å¯èƒ½å› æ¢ç´¢æ¦‚ç‡ä½è€Œéš¾å‘ç°é«˜å¥–åŠ±çŠ¶æ€ï¼‰ã€‚

**Rmax**

å‡è®¾$q_{\pi}(s,a)=R_{max}$MDP æœ€å¤§å³æ—¶å¥–åŠ±ï¼‰ï¼Œé™¤éè¡ŒåŠ¨åœ¨çŠ¶æ€è‡³å°‘è¢«é‡‡å–æ¬¡ï¼Œè¿«ä½¿æ™ºèƒ½ä½“å¤šæ¬¡å°è¯•æ‰€æœ‰å¯èƒ½è¡ŒåŠ¨å†åšç»“è®ºã€‚ä¼˜ç‚¹æ˜¯è‹¥è¶³å¤Ÿå¤§ï¼Œåœ¨æ— é™é•¿å­¦ä¹ è¿‡ç¨‹ä¸­å¤§æ¦‚ç‡æœ€ä¼˜ï¼ˆæ ·æœ¬å¤æ‚åº¦ç†è®ºï¼‰ï¼Œç¼ºç‚¹æ˜¯æ¢ç´¢è¿‡äºæ¿€è¿›ï¼Œåœ¨ä¸€äº›å®é™…åº”ç”¨ï¼ˆå¦‚æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€ç”µç«™æ§åˆ¶ç­‰ï¼‰å¯èƒ½ä¸åˆé€‚ï¼ŒçŠ¶æ€ / è¡ŒåŠ¨ç©ºé—´å¤§æ—¶å¯èƒ½èŠ±è´¹å¤ªå¤šæ—¶é—´æ¢ç´¢è€Œæ— åˆç†è®¡åˆ’ï¼ˆå¤šæ•°å€¼ä¸º$R_{max}$ï¼‰ã€‚

**Choose a strategy**

- é€‰æ‹©ç­–ç•¥åº”æ ¹æ®åº”ç”¨éœ€æ±‚ï¼Œå¦‚ç°å®æŸå¤±å¯å¿½ç•¥ï¼ˆæ¸¸æˆç­‰ï¼‰å¯é‡‡ç”¨ç³»ç»Ÿæ¢ç´¢ï¼Œå®éªŒæ˜‚è´µï¼ˆæœºå™¨äººç­‰ï¼‰åˆ™é‡‡ç”¨æ›´ä¿å®ˆï¼ˆè´ªå©ªï¼‰æ¢ç´¢ã€‚

### Model-free RL

Model freeçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸å°è¯•å¯¹ç¯å¢ƒè¿›è¡Œæ˜¾å¼å»ºæ¨¡ï¼Œè€Œæ˜¯ç›´æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥æˆ–ä»·å€¼å‡½æ•°

<img src="AAIå¤ä¹ _image/image-20250102154408534.png" alt="image-20250102154408534" style="zoom:50%;" />

> G(t)ä¸ºæŠ˜æ‰£çš„ç´¯åŠ å¥–åŠ±å›æŠ¥

**MC vs TD**

MC: unbiased, but usually has a higher variance

TD: biased, but usually has a lower variance

- On-policy: estimated values $\hat{q}^{\pi}$ is about$ \pi$
  - TD, Sarsa, MC

- Off-policy: estimated values  $\hat{q}^{\pi}$is NOT about ğœ‹, but about some other (possibly better) policy
  - Q-learning

<img src="AAIå¤ä¹ _image/image-20250102155331366.png" alt="image-20250102155331366" style="zoom:50%;" />